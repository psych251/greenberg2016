---
title: "Greenberg2016 Replication"
output:
  html_document: default
  html_notebook: default
---

### Replication of Study "Opportunity Cost Neglect Attenuates the Effect of Choices on Preferences" by Adam Eric Greenberg & Stephen A. Spiller (2016, Psychological Science)

Gobi Dasu
gdasu@stanford.edu

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

```{r, include=FALSE}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
library(stringr)
require(broom) # for tidy()
```

#Introduction

People often forget to consider opportunity costs when making decisions. So, Greenberg and Spiller (2016) looked at how much people's evaluation of available options changes when opportunity costs are made explicit at the time of choice. They hypothesized that people would value options they selected (over a default) as higher than the default opportunity cost. Moreover they thought that the distance between people's evaluation of options they chose (over defaults) and their evaluation of a default opportunity cost would be greater when opportunity cost is made explicit at time of choice. In other words, when we are aware of opportunity costs, we value our intentional choices as particularly better than default options. Formally, the authors looked at how making opportunity cost explicit at the time of choice affects evaluations of focal options ("available" options) and the opportunity cost itself. The study tested the difference in evaluations of options after the experience of whether-or-not (explicit opportunity cost consideration condition) questions versus which-one (implicit condition) questions. This research offers readers a detailed understanding of the psychology of everyday choices, particularly with respect to a notion of opportunity cost which exists but which people are not always made aware of when making decisions. 

To test this change in the evaluation of options across conditions, the researchers looked at how "spread" of preferences across choices is affected by making opportunity cost explicit. The researchers calculated the spread of pre-choice and post-choice responses by taking the mean of the focal option evaluations minus the base option (opportunity cost) evaluation. Spread could be negative or positive, and just represented how far away mean focal option evaluation was from the evaluation of opportunity cost. Based on this definition, one may expect that a participant in the explicit condition who selects many focal options explicitly over the opportunity cost base option, would in a post-choice survey evaluate their chosen focal options as even further away from the base option than in their pre-choice survey. To test this, the researchers hence controlled for pre-choice spread and used proportion of focal options chosen and condition as well as their interaction to predict post-choice spread. The interaction of condition and proportion of focal options chosen indeed was a significant predictor of post-choice spread (p=0.023). This was the key statistic. Moreover, they found that when opportunity costs were explicit, postchoice spread increased with the proportion of focal options chosen (p=0.006), but no significant relationship when the condition was implicit. 

This was a successful replication. We ran the experiment (N=28) described by the authors and like the authors, found that the interaction of condition and proportion of focal options chosen indeed was a significant predictor of post-choice spread (p=0.036). Moreover, we found that when opportunity costs were explicit, postchoice spread increased with the proportion of focal options chosen (p=0.021), but no significant relationship when the condition was implicit. 

#Methods

Here is a link to the current working experiment: https://stanforduniversity.qualtrics.com/jfe/form/SV_4OWrURMHT8ffmHr

###Power Analysis
R^2 (population squared multiple correlation) is the effect size of a multiple linear regression. After contacting the authors, we were told that observed R^2 for the full model was 0.5621. Adjusted R^2 for the full model was 0.5439. Full model test was F(4, 96) = 30.81, p < .001. Given the sample size of 101, the 3 predictors (1 of which was a tested predictor -- choice), and the pre-specified significance level of alpha err prob = 0.05, we found that the post-hoc power of the experiment was 0.99.

For the model to detect the effect size of R^2 = 0.5621 (f^2 = 1.284), with an alpha err prob = 0.05, we found 10 samples were required to achieve 80% power, 12 samples for 90% power, and 13 samples for 95% power. We used G*Power.

###Planned Sample
Because of the sufficiently large effect size, we planned to use fewer samples than the experimenters did for sake of cost -- 35 participants, enough for 99% power. We opted for this very high fidelity replication because the task was not very long (only 5 minutes based on our pilot). Like the original experimenters, we sampled US-only mechanical turk workers (80%+ approval rating) and applied no other preselection rules. We threw out samples that failed the attention check and ensured that the difference in comprehension check performance between the two groups was not significant. The attention check was simply a dummy question that asked participants to select "somewhat disagree" and not what they actually felt regarding a question that asked "Do you agree with the following: I generally feel positive about receiving gift cards". The comprehension check tested whether participants knew that they would get a $100 VISA gift card if they did not spend the 50K miles they were originally given in the experiment. 

###Materials & Procedure
Quoted text from the paper follows:

> We assessed prechoice evaluations by asking participants to imagine that they were eligible to receive different independent offers as part of a marketing promotion (e.g., “2 nights in a luxury hotel”). Participants used a 7-point scale (1 = not at all, 7 = very much) to rate how much they would like each of 11 such offers. One of these (a $100 Visa gift card) would be the option that represented the opportunity cost at the time of choice and was either the first offer or the last offer to be evaluated; order was counterbalanced across participants.
After completing the prechoice evaluations, participants were asked to imagine that they had accumulated 50,000 airline miles and that their miles would expire soon. If they did not spend their miles before the miles expired, the airline would send them a $100 Visa gift card. Participants decided whether to spend their airline miles on each of 10 focal offers (e.g., “Spend my miles on 2 nights in a luxury hotel”).2 Each option was presented as an independent decision, and participants were instructed to treat each decision as though it were their only opportunity to spend their miles before they expired. As a result, the 10 focal offers competed not with one another but only with the opportunity cost (i.e., the $100 Visa gift card).
Participants assigned to the implicit condition made choices between accepting the focal options (e.g., “Spend my miles on 2 nights in a luxury hotel”) or rejecting the focal options (e.g., “Do not spend my miles on 2 nights in a luxury hotel”); this frame made opportunity costs implicit. Participants assigned to the explicit condition made choices between accepting the focal options or rejecting the focal options in favor of the explicit opportunity cost instead (e.g., “Do not spend my miles on 2 nights in a luxury hotel and take the $100 Visa gift card instead”; Frederick et al., 2009). The two frames are formally equivalent.
Postchoice evaluations were assessed by asking par- ticipants again to rate the desirability of each of the 11 prechoice offers, using a method identical to that used in the prechoice evaluations. The order in which the oppor- tunity cost was presented was counterbalanced. At the top of the survey screen, participants saw the full list of 10 choices they had made (e.g., “Given the choice of: [focal option], or [nonfocal option], you chose: [chosen option]”). The wording was consistent with the manipu- lation: The description of the nonfocal option included the phrase “and take the $100 Visa gift card instead” only for participants in the explicit condition.
Finally, participants reported what they thought would happen to their miles if they decided not to spend them, completed an attention check, and reported their sex and age."

Our Materials and Procedure were identical to the authors', quoted above.

###Analysis Plan
Firstly, data was anonymized and those who did not pass the attention check were removed. We ensured that the proportion of participants passing the comprehension check did not vary by condition, as did the authors. 

The researchers calculated the spread of pre-choice and post-choice responses by taking the mean of the focal option evaluations minus the base option (opportunity cost) evaluation. They regressed post-choice spread on condition (implicit or explicit) and proportion of focal offers chosen as well as the interaction of the two, controlling for pre-choice spread. They also performed "simple effects" linear regressions of (a) post choice spread [Y] VERSUS proportion of focal options chosen [X], (b) evaluation of focal options [Y] VERSUS proportion of focal options chosen [X], and (c) evaluation of opportunity costs [Y] VERSUS proportion of focal options chosen [X]. In (a), prechoice spread was a covariate, but in (b) & (c), only the relevant prechoice evaluation was used as a covariate. Specifically, mean prechoice evaluation of focal options was a covariate for (b), and prechoice evaluation of the opportunity cost was a covariate for (c). The exact simple effects regression formulae were not obvious from the paper, so we consulted the authors who provided us the exact implementation, which we use below.

###Differences from Original Study
There were no differences between our analysis methods and those of the original study.

#(Post Data Collection) Methods Addendum

###Actual Sample
Note, 28 of the 35 recruited participants ended up passing the attention check the authors designed but this number of participants was still a sufficient sample size for achieving 99% power. 15 participants were Male and 13 particiants were female. They ranged in age from 20 to 47.

###Differences from pre-data collection methods plan
None

#Results

###Data preparation
We performed data preparation following the analysis plan. Some code has been hidden here for aesthetic purposes, but you can view all the code if you open greenberg-repl.Rmd from https://github.com/psych251/greenberg2016 using R Studio. 

```{r, echo=FALSE}

# NOTE! qualtrics download wasn't ideal so had to do Find&Delete of ": Very Much" / ": Not at all" string annotations that were cluttering numerical data
qualtrics_data <- read_csv("final/final_anonymized.csv") %>% 
  rename(consent=Q1) %>%
  rename(prechoice_100VISA_1=Q4) %>% # prechoice
  rename(prechoice_focal_1st=Q7) %>%
  rename(prechoice_focal_last=Q16) %>%
  rename(prechoice_100VISA_2=Q69) %>% 
  rename(choice_explicit_1st=Q18) %>% # choice explicit
  rename(choice_explicit_last=Q27) %>% 
  rename(postchoice_explicit_100VISA_1=Q41) %>%
  rename(postchoice_explicit_focal_1st=Q42) %>%
  rename(postchoice_explicit_focal_last=Q51) %>%
  rename(postchoice_explicit_100VISA_2=Q70) %>%
  rename(choice_implicit_1st=Q30) %>% # choice implicit
  rename(choice_implicit_last=Q39) %>% 
  rename(postchoice_implicit_100VISA_1=Q53) %>%
  rename(postchoice_implicit_focal_1st=Q54) %>%
  rename(postchoice_implict_focal_last=Q63) %>%
  rename(postchoice_implicit_100VISA_2=Q71) %>%
  rename(comprehension_check=Q65) %>%
  rename(attention_check=Q66) %>%
  rename(sex=Q67) %>%
  rename(age=Q68)
original_data <- read_csv("GreenbergMaterials/GreenbergSpillerExp1-ourstyle.csv") %>%
  rename(consent=Q1) %>%
  rename(prechoice_100VISA_1=Q4) %>% # prechoice
  rename(prechoice_focal_1st=Q7) %>%
  rename(prechoice_focal_last=Q16) %>%
  rename(prechoice_100VISA_2=Q69) %>% 
  rename(choice_explicit_1st=Q18) %>% # choice explicit
  rename(choice_explicit_last=Q27) %>% 
  rename(postchoice_explicit_100VISA_1=Q41) %>%
  rename(postchoice_explicit_focal_1st=Q42) %>%
  rename(postchoice_explicit_focal_last=Q51) %>%
  #rename(postchoice_explicit_100VISA_2=Q70) %>%
  rename(choice_implicit_1st=Q30) %>% # choice implicit
  rename(choice_implicit_last=Q39) %>% 
  rename(postchoice_implicit_100VISA_1=Q53) %>%
  rename(postchoice_implicit_focal_1st=Q54) %>%
  rename(postchoice_implict_focal_last=Q63) %>%
  #rename(postchoice_implicit_100VISA_2=Q71) %>%
  rename(comprehension_check=Q65) %>%
  rename(attention_check=Q66) %>%
  rename(sex=Q67) %>%
  rename(age=Q68)

# remove 1st 2 rows
qualtrics_data <- qualtrics_data[-c(1,2),]

# calculate prechoice spread, postchoice spread, proportion of focal options chosen, condition
qualtrics_data <- qualtrics_data %>% 
  mutate(condition = ifelse(is.na(qualtrics_data$choice_explicit_1st), -1, 1)) %>%
  mutate(allchoices = do.call(paste0, qualtrics_data[c(31:40, 53:62)])) 
original_data <- original_data %>% 
  mutate(condition = ifelse(condition=="No OC", -1, 1)) %>%
  mutate(allchoices = do.call(paste0, original_data[c(34:43, 45:54)])) 


qualtrics_data$prop_focal_chosen <- (10 - str_count(qualtrics_data$allchoices, "Not spend"))/10
original_data$prop_focal_chosen <- (10 - str_count(original_data$allchoices, "2"))/10

qualtrics_data[, c(63:74)] <- sapply(qualtrics_data[, c(63:74)], as.numeric)
qualtrics_data[, c(41:52)] <- sapply(qualtrics_data[, c(41:52)], as.numeric)
original_data[, c(58:69)] <- sapply(original_data[, c(58:69)], as.numeric)
original_data[, c(71:82)] <- sapply(original_data[, c(71:82)], as.numeric)

qualtrics_data$postchoice_spread <- ifelse(
  qualtrics_data$condition==-1, 
  rowMeans(subset(qualtrics_data, select = c(64:73))) - 
    ifelse(
      is.na(qualtrics_data$postchoice_implicit_100VISA_1),
      qualtrics_data$postchoice_implicit_100VISA_2,
      qualtrics_data$postchoice_implicit_100VISA_1
    ),
  rowMeans(subset(qualtrics_data, select = c(42:51))) - 
    ifelse(
      is.na(qualtrics_data$postchoice_explicit_100VISA_1),
      qualtrics_data$postchoice_explicit_100VISA_2,
      qualtrics_data$postchoice_explicit_100VISA_1
    )
)
original_data$postchoice_spread <- ifelse(
  is.na(original_data[59]),
  rowMeans(subset(original_data, select = c(72:81))),
  rowMeans(subset(original_data, select = c(59:68)))
) - ifelse(
  is.na(original_data$postchoice_explicit_100VISA_1), # explicit/implicit NOT good name here at all
  original_data$postchoice_implicit_100VISA_1, # just referring to the counterbalanced 1st or 2nd
  original_data$postchoice_explicit_100VISA_1 # value of the evaluation of the OC (VISA card) 
)

qualtrics_data$focal_opts_eval_post = ifelse(
  qualtrics_data$condition==-1, 
  rowMeans(subset(qualtrics_data, select = c(64:73))),
  rowMeans(subset(qualtrics_data, select = c(42:51)))
)
original_data$focal_opts_eval_post = ifelse(
  is.na(original_data[59]), 
  rowMeans(subset(original_data, select = c(72:81))),
  rowMeans(subset(original_data, select = c(59:68)))
)

qualtrics_data$opp_cost_eval_post = ifelse(
  qualtrics_data$condition==-1, 
  ifelse(
    is.na(qualtrics_data$postchoice_implicit_100VISA_1),
    qualtrics_data$postchoice_implicit_100VISA_2,
    qualtrics_data$postchoice_implicit_100VISA_1
  ),
  ifelse(
    is.na(qualtrics_data$postchoice_explicit_100VISA_1),
    qualtrics_data$postchoice_explicit_100VISA_2,
    qualtrics_data$postchoice_explicit_100VISA_1
  )
)
original_data$opp_cost_eval_post = ifelse(
  is.na(original_data$postchoice_explicit_100VISA_1), # explicit/implicit NOT good name here at all
  original_data$postchoice_implicit_100VISA_1, # just referring to the counterbalanced 1st or 2nd
  original_data$postchoice_explicit_100VISA_1 # value of the evaluation of the OC (VISA card) 
)

qualtrics_data[, c(19:30)] <- sapply(qualtrics_data[, c(19:30)], as.numeric)
original_data[, c(11:20)] <- sapply(original_data[, c(11:20)], as.numeric)

qualtrics_data$prechoice_spread <- rowMeans(subset(qualtrics_data, select = c(20:29))) - 
  ifelse(
    is.na(qualtrics_data$prechoice_100VISA_1),  
    qualtrics_data$prechoice_100VISA_2,
    qualtrics_data$prechoice_100VISA_1
  )
original_data$prechoice_spread <- rowMeans(subset(original_data, select = c(11:20))) - 
  ifelse(
    is.na(original_data$prechoice_100VISA_1),  
    original_data$prechoice_100VISA_2,
    original_data$prechoice_100VISA_1
  )

qualtrics_data$focal_opts_eval_pre <- rowMeans(subset(qualtrics_data, select = c(20:29)))
original_data$focal_opts_eval_pre <- rowMeans(subset(original_data, select = c(11:20)))

qualtrics_data$opp_cost_eval_pre <- ifelse(
  is.na(qualtrics_data$prechoice_100VISA_1),  
  qualtrics_data$prechoice_100VISA_2,
  qualtrics_data$prechoice_100VISA_1
)
original_data$opp_cost_eval_pre <- ifelse(
  is.na(original_data$prechoice_100VISA_1),  
  original_data$prechoice_100VISA_2,
  original_data$prechoice_100VISA_1
)

d_pilotA <- qualtrics_data[c(7:9,15:47), ] 
d_pilotB <- qualtrics_data[c(72:90), ]
d_final <- qualtrics_data[c(94:128), ]
d_original <- original_data

# make sure attention check passed
d_pilotA <- d_pilotA[d_pilotA$attention_check=='Somewhat disagree',]
d_pilotB <- d_pilotB[d_pilotB$attention_check=='Somewhat disagree',] %>% mutate(ImplicitIs0=(condition==1)) %>% mutate(ExplicitIs0=(condition==-1))
d_final <- d_final[d_final$attention_check=='Somewhat disagree',] %>% mutate(ImplicitIs0=(condition==1)) %>% mutate(ExplicitIs0=(condition==-1))
d_original <- d_original[d_original$attention_check==1,] %>% mutate(ImplicitIs0=(condition==1)) %>% mutate(ExplicitIs0=(condition==-1))
```


###Confirmatory analysis
We procede to present the analyses as specified in the analysis plan. 

Before doing a "replication", we did a "reproduction" and two pilots. For the reproduction, we used open data from the original experiment to perform all analyses, and achieved the exact same results as the authors. For the first pilot we used data from friends and peers. For the second pilot we ran a small scale study on mechanical turk. The analyses for all 3 of these preparatory measures are presented in the Appendix. 

```{r}
fit <- lm(postchoice_spread ~ prechoice_spread + condition*prop_focal_chosen, data=d_final)
kable(tidy(fit))
```

We successfully replicated the finding that the condition:prop_focal_chosen interaction is a significant predictor (p=0.036) of postchoice_spread. 

We also implemented, for final data, the remaining auxiliary analysis of the paper, such as simple effects models and plots of simple effects models. The following regression formulae were explicitly provided by the authors.

```{r}
d_explicit <- d_final[d_final$condition==1,]
d_implicit <- d_final[d_final$condition==-1,]

fit <- lm(postchoice_spread ~ prechoice_spread + prop_focal_chosen + ImplicitIs0 + prop_focal_chosen:ImplicitIs0, data=d_final) # for IMPLICIT SLOPE
kable(tidy(fit))
fit <- lm(postchoice_spread ~ prechoice_spread + prop_focal_chosen + ExplicitIs0 + prop_focal_chosen:ExplicitIs0, data=d_final) # for EXPLICIT SLOPE
kable(tidy(fit))
```

We found that prop_focal_chosen predicts postchoice_spread in the explicit (p=0.021) but not implicit (p=0.943) conditions, which matches the paper in which prop_focal_chosen predicts postchoice_spread only in the explicit condition. 

```{r}
fit <- lm(focal_opts_eval_post ~ focal_opts_eval_pre + prop_focal_chosen + ImplicitIs0 + prop_focal_chosen:ImplicitIs0, data=d_final) # IMPLICIT SLOPE
kable(tidy(fit))
fit <- lm(focal_opts_eval_post ~ focal_opts_eval_pre + prop_focal_chosen + ExplicitIs0 + prop_focal_chosen:ExplicitIs0, data=d_final) # EXPLICIT SLOPE
kable(tidy(fit))
```

We found that prop_focal_chosen predicts evaluation of focal options in neither condition, which does not match the paper in which prop_focal_chosen predicts postchoice_spread in both conditions. Note, it seems like focal_opts_eval_pre carries all the predictive power in this regression.

The researchers decided to plot the above relationship focal_opts_eval_post VS prop_focal_chosen (plot in the Reproduction in the Appendix for the authors' plot but also included below) so we do the same.

```{r, echo=FALSE}
tofit <- data.frame(AvgChoice=numeric(), LBase=numeric(), LBaseLwr=numeric(), LBaseUpr=numeric())
for (i in 0:10) {
  tofit[nrow(tofit) + 1,] = c(i*0.1, NULL, NULL, NULL)
}
tofit_implicit <- tofit %>% mutate(LBase=0.4728*AvgChoice + 0.4515)  # from regression above

# THIS STYLE DID NOT WORK -- STUCK TO MY SIMPLE EFFECTS PLOTS
```

```{r, echo=FALSE}
d_final$condition <-as.factor(d_final$condition)
condition_names <- list("-1"="Implicit Condition","1"="Explicit Condition")
condition_labeller <- function(variable,value){ return(condition_names[value]) }
  
ggplot(data = d_final, aes(x=prop_focal_chosen, y=focal_opts_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Focal Options") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") + facet_grid(.~condition, labeller=condition_labeller)
```

This plot actually looks quite similar to the results that the authors got for explicit and implicit condition. The shaded area is the 95% confidence interval, same as what the authors graphed. 

To ease comparison, we also include the reproduction of the author's plot below:

```{r, echo=FALSE}
d_original$condition <-as.factor(d_original$condition)
ggplot(data = d_original, aes(x=prop_focal_chosen, y=focal_opts_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Focal Options") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") + facet_grid(.~condition, labeller=condition_labeller)
```

Now we proceed to the next simple effects model: 

```{r}
fit <- lm(opp_cost_eval_post ~ opp_cost_eval_pre + prop_focal_chosen + ImplicitIs0 + prop_focal_chosen:ImplicitIs0, data=d_final) # IMPLICIT SLOPE
kable(tidy(fit))
fit <- lm(opp_cost_eval_post ~ opp_cost_eval_pre + prop_focal_chosen + ExplicitIs0 + prop_focal_chosen:ExplicitIs0, data=d_final) # EXPLICIT SLOPE
kable(tidy(fit))
```

We find that prop_focal_chosen predicts opp_cost_eval_post for the explicit slope condition but not for the implicit condition which is the opposite of what the researchers in the paper got. Note that the prechoice evaluation is still the main variable of great predictive power.

The researchers decided to plot the above relationship opp_cost_eval_post VS prop_focal_chosen (see the Reproduction in the Appendix for the authors' plot but also available below) so we do the same: 

```{r, echo=FALSE}
ggplot(data = d_final, aes(x=prop_focal_chosen, y=opp_cost_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Opportunity Cost") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") + facet_grid(.~condition, labeller=condition_labeller)
```

Similar to the authors, we also get a downward slope for the explicit condition for the graph of Eval of Opportunity Cost vs Prop of Focal Options Chosen. And like the authors, we get an upward slope for the plot of Eval of Opportunity Cost vs. Proportion of Focal Options Chosen. However the slopes are not the same between the researchers' findings and ours.  

To ease comparison, we also include the reproduction of the author's plot below:

```{r, echo=FALSE}
ggplot(data = d_original, aes(x=prop_focal_chosen, y=opp_cost_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Opportunity Cost") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") + facet_grid(.~condition, labeller=condition_labeller)
```

We get similar plots to the authors but don't always agree on which variables are significant predictors in the simple effects models. We through this may be because our prechoice evaluations carry substantial predictive power.

###Exploratory analyses

We wanted to visualize the postchoice_spread VS prop_focal_chosen plot for each condition, which the authors did not present. 

Here are those plots for the data we collected:

Explicit:

```{r, echo=FALSE}
ggplot(data = d_explicit, aes(x=prop_focal_chosen, y=postchoice_spread)) + geom_point() + xlim(c(0,1.0)) + ylim(c(-6,6)) + ylab("Postchoice Spread") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

Implicit:

```{r, echo=FALSE}
ggplot(data = d_implicit, aes(x=prop_focal_chosen, y=postchoice_spread)) + geom_point() + xlim(c(0,1.0)) + ylim(c(-6,6)) + ylab("Postchoice Spread") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

And here are those plots for the original data:

Explicit:

```{r, echo=FALSE}
ggplot(data = d_original[d_original$condition==1,], aes(x=prop_focal_chosen, y=postchoice_spread)) + geom_point() + xlim(c(0,1.0)) + ylim(c(-6,6)) + ylab("Postchoice Spread") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm")
```

Implicit:

```{r, echo=FALSE}
ggplot(data = d_original[d_original$condition==-1,], aes(x=prop_focal_chosen, y=postchoice_spread)) + geom_point() + xlim(c(0,1.0)) + ylim(c(-6,6)) + ylab("Postchoice Spread") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm")
```

So we can visualize clearly here how much making opportunity cost explicit affects spread of the evaluation of options (i.e. spread away from evaluation of the opportunity cost). In the explicit condition, we witness a rise in spread as a participant chooses more focal options, where as in the implicit condition we do not. 

We saw in some of our simple effects models (for predicting "evaluation of focal options" and "evaluation of opportunity cost"), that the relevant prechoice evaluation carried most of the predictive power. For this reason, we decided to look at the plot of the simple effect dependent variable VERSUS the relevant prechoice control variable to inspect whether the postchoice evals were almost exactly trailing the prechoice evals in our data, but a bit less so for the original data.

Here are the plots for our data: 

```{r, echo=FALSE}
ggplot(data = d_final, aes(x=focal_opts_eval_pre, y=focal_opts_eval_post)) + geom_point() + ylab("Evaluation of Focal Options") + xlab("Prechoice Evaluation of Focal Options") + geom_smooth(method="lm") 
```


```{r, echo=FALSE}
ggplot(data = d_final, aes(x=opp_cost_eval_pre, y=opp_cost_eval_post)) + geom_point() + ylab("Evaluation of Opportunity Cost") + xlab("Prechoice Evaluation of Opportunity Cost") + geom_smooth(method="lm") 
```


And here are the plots for the original data:


```{r, echo=FALSE}
ggplot(data = d_original, aes(x=focal_opts_eval_pre, y=focal_opts_eval_post)) + geom_point() + ylab("Evaluation of Focal Options") + xlab("Prechoice Evaluation of Focal Options") + geom_smooth(method="lm") 
```


```{r, echo=FALSE}
ggplot(data = d_original, aes(x=opp_cost_eval_pre, y=opp_cost_eval_post)) + geom_point() + ylab("Evaluation of Opportunity Cost") + xlab("Prechoice Evaluation of Opportunity Cost") + geom_smooth(method="lm") 
```


We see from the plots above (note y axis ranges) that postchoice evaluations track prechoice evaluations both in our data and the original study, so this exploratory analysis did not display any extra "post evaluation tracking pre evaluation" trend in our data that was not present in the original data. 

#Discussion

###Summary of Replication Attempt
This replication was a success because the target finding that the interaction between condition (implicit vs explicit) and proportion of focal options chosen is a significant predictor of postchoice spread (p=0.0360). Moreover, this replication was able to confirm that in the explicit but not implicit condition, proportion of focal options predicts postchoice spread (p=0.021).

###Commentary
The replication was an overall success -- the primary target findings were successfully replicated.  However, significance statistics in auxiliary analyses of simple effects that predicted postchoice focal option evaluation or postchoice opportunity cost evaluation did not match between our data and the original dataset. 

The successful reproduction from original open data (see Appendix) made us confident about the analyses we were running, and the matching target findings in the replication made us confident about our experiment itself. Exploratory analyses even gave us a good visualization of the difference between "postchoice spread VS prop_focal_chosen" in the explicit and implicit conditions. So it came as a surprise that the latter two of our three simple effects models did not fully match up with those reported by the authors. Exploratory analysis showed us that postchoice evaluations (of opp cost or focal options) trailed prechoice evaluations both in our data and in the original data. So we could not visually confirm our guess that stronger predictive power of the prechoice evaluations in our data than in the authors' was the reason behind different variables being significant in the second and third simple effects models. 

In the simple effects model predicting post choice opportunity cost evaluation, we found that prop_focal_chosen predicts opp_cost_eval_post for the explicit slope condition but not for the implicit condition which is the opposite of what the researchers in the paper got. Even though we agree the interaction between condition and prop_focal_chosen predicts postchoice_spread, this finding calls into question the underlying mechanism behind what is the driving variable under different conditions. However, we cannot make further claims using our data because we designed our experiment to be well powered for the target finding (the key statistic), not the simple effects regressions. 

Even though we calculated power for the multiple regression, perhaps matching the simple effects would require an alternate G*power analysis since the simple effects are different regressions. To match the simple effects, more data from a larger experiment may be needed. The authors reviewed our implementation and had no objections to our final study.

#Appendix

##Reproduction from Open Data provided by Researchers

```{r}
fit <- lm(postchoice_spread ~ prechoice_spread + condition*prop_focal_chosen, data=d_original)
kable(tidy(fit))
```

With original data, we successfully reproduced the finding that the condition:prop_focal_chosen interaction is a significant predictor of postchoice_spread. The p-values and coefficients are exactly the same as what the researchers found.

We also implemented, for original data, the remaining auxiliary analysis of the paper, such as simple effects models and plots of simple effects models. Please note, ImplicitIs0 is an encoding of the condition variable that the authors used, where the implicit condition was encoded as a 0 and the explicit as a 1. ExplicitIs0 is an encoding of the condition variable that the authors used, where the explicit condition was encoded as a 0 and the implicit as a 1.

```{r}
d_explicit <- d_original[d_original$condition==1,]
d_implicit <- d_original[d_original$condition==-1,]

fit <- lm(postchoice_spread ~ prechoice_spread + prop_focal_chosen + ImplicitIs0 + prop_focal_chosen:ImplicitIs0, data=d_original) # to plot: coeff on prop_focal_chosen # IMPLICIT SLOPE
kable(tidy(fit))
fit <- lm(postchoice_spread ~ prechoice_spread + prop_focal_chosen + ExplicitIs0 + prop_focal_chosen:ExplicitIs0, data=d_original) # to plot: coeff on prop_focal_chosen # EXPLICIT SLOPE
kable(tidy(fit))
```

We get that prop_focal_chosen predicts postchoice_spread in the explicit but not implicit conditions, which matches the paper in which prop_focal_chosen predicts postchoice_spread only in the explicit condition. Moreover, we find the exact same p-values and coefficients as the authors.

```{r}
fit <- lm(focal_opts_eval_post ~ focal_opts_eval_pre + prop_focal_chosen + ImplicitIs0 + prop_focal_chosen:ImplicitIs0, data=d_original) # to plot: coeff on prop_focal_chosen # IMPLICIT SLOPE
kable(tidy(fit))
fit <- lm(focal_opts_eval_post ~ focal_opts_eval_pre + prop_focal_chosen + ExplicitIs0 + prop_focal_chosen:ExplicitIs0, data=d_original) # to plot: coeff on prop_focal_chosen # EXPLICIT SLOPE
kable(tidy(fit))
```

Again, we find the exact same significance relationship, p-values, and coefficients as the authors.

The researchers decided to plot the above relationship focal_opts_eval_post VS prop_focal_chosen so we do the same.

For the Explicit condition: 

```{r, echo=FALSE}
ggplot(data = d_explicit, aes(x=prop_focal_chosen, y=focal_opts_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Focal Options") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

This is the same graph as what the authors got.

And for the implicit condition:

```{r, echo=FALSE}
ggplot(data = d_implicit, aes(x=prop_focal_chosen, y=focal_opts_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Focal Options") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

This is the same graph as what the authors got.

We also look at opportunity cost evaluation VS proportion of focal options chosen, as the authors do.

```{r}
fit <- lm(opp_cost_eval_post ~ opp_cost_eval_pre + prop_focal_chosen + ImplicitIs0 + prop_focal_chosen:ImplicitIs0, data=d_original) # to plot: coeff on prop_focal_chosen # IMPLICIT SLOPE because when ImplicitIs0 is 1 then 
kable(tidy(fit))
fit <- lm(opp_cost_eval_post ~ opp_cost_eval_pre + prop_focal_chosen + ExplicitIs0 + prop_focal_chosen:ExplicitIs0, data=d_original) # to plot: coeff on prop_focal_chosen # EXPLICIT SLOPE
kable(tidy(fit))
```

Again, we find the exact same significance relationship, p-values, and coefficients as the authors.

The researchers decided to plot the above relationship opp_cost_eval_post VS prop_focal_chosen so we do the same: 

For the Explicit condition:

```{r, echo=FALSE}
ggplot(data = d_explicit, aes(x=prop_focal_chosen, y=opp_cost_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Opportunity Cost") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

Similar to the authors, we also get a downward slope for the explicit condition for the graph of Eval of Opportunity Cost vs Prop of Focal Options Chosen. 

For the Implicit condition:

```{r, echo=FALSE}
ggplot(data = d_implicit, aes(x=prop_focal_chosen, y=opp_cost_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Opportunity Cost") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

Like the authors, we get an upward slope for the plot of Eval of Opportunity Cost vs. Proportion of Focal Options Chosen. 

So the original data matches up with the paper. The "reproduction" was a success.

##PILOT A
```{r}
fit <- lm(postchoice_spread ~ prechoice_spread + condition*prop_focal_chosen, data=d_pilotA)
kable(tidy(fit))
```

Like the authors, here we see that PRECHOICE_SPREAD has a p < .001.
Like the authors, we see that PROP_FOCAL_OPT_CHOSEN is significant (their_p = .021, our_p=.044).
Strangely, our CONDITION and PROP_FOCAL_OPT_CHOSEN:CONDITION p values are exactly 10 times larger than the authors' p=.018 and p=.023. We went through and could not catch any error in our implementation -- we have implemented this pipeline in both excel and R independently, for instance, to ensure these analyses are valid.

## PILOT B
```{r}
fit <- lm(postchoice_spread ~ prechoice_spread + condition*prop_focal_chosen, data=d_pilotB)
kable(tidy(fit))
```

Again, like in Pilot A, we see that PRECHOICE_SPREAD and PROP_FOCAL_OPT_CHOSEN are significant predictors of POSTCHOICE_SPREAD, but CONDITION and the interaction of CONDITION and PROP_FOCAL_OPT_CHOSEN are not significant predictors, though the paper claimed they should be. 

I also implemented, for pilot B data, the remaining auxiliary analysis of the paper, such as simple effects models and plots of simple effects models:

```{r}
d_explicit <- d_pilotB[d_pilotB$condition==1,]
d_implicit <- d_pilotB[d_pilotB$condition==-1,]

fit <- lm(postchoice_spread ~ prechoice_spread + prop_focal_chosen + ImplicitIs0 + prop_focal_chosen:ImplicitIs0, data=d_pilotB) 
kable(tidy(fit))
fit <- lm(postchoice_spread ~ prechoice_spread + prop_focal_chosen + ExplicitIs0 + prop_focal_chosen:ExplicitIs0, data=d_pilotB)
kable(tidy(fit))
```

We get that prop_focal_chosen predicts postchoice_spread in the implicit and explicit conditions, which does not match the paper in which prop_focal_chosen predicts postchoice_spread only in the explicit condition.

```{r}
fit <- lm(focal_opts_eval_post ~ focal_opts_eval_pre + prop_focal_chosen + ImplicitIs0 + prop_focal_chosen:ImplicitIs0, data=d_pilotB)
kable(tidy(fit))
fit <- lm(focal_opts_eval_post ~ focal_opts_eval_pre + prop_focal_chosen + ExplicitIs0 + prop_focal_chosen:ExplicitIs0, data=d_pilotB)
kable(tidy(fit))
```

We find that prop_focal_chosen predicts focal_opts_eval_post for the implicit condition but not for the explicit condition which contradicts the authors' findings that prop_focal_chosen predicts focal_opts_eval_post in both conditions.

The researchers decided to plot the above relationship focal_opts_eval_post VS prop_focal_chosen so we do the same: 

For the Explicit condition: 

```{r, echo=FALSE}
ggplot(data = d_explicit, aes(x=prop_focal_chosen, y=focal_opts_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Focal Options") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

This plot actually looks quite similar to the results that the authors got for explicit condition. The shaded area is the 95% confidence interval, same as what the authors graphed. 

And for the implicit condition:

```{r, echo=FALSE}
ggplot(data = d_implicit, aes(x=prop_focal_chosen, y=focal_opts_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Focal Options") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

This plot is similar to the results that the authors got for implicit condition, but it seems like we do not have enough data to cover the full graph. The shaded area is the 95% confidence interval, same as what the authors graphed. 

```{r}
fit <- lm(opp_cost_eval_post ~ opp_cost_eval_pre + prop_focal_chosen + ImplicitIs0 + prop_focal_chosen:ImplicitIs0, data=d_pilotB) # to plot: coeff on prop_focal_chosen
kable(tidy(fit))
fit <- lm(opp_cost_eval_post ~ opp_cost_eval_pre + prop_focal_chosen + ExplicitIs0 + prop_focal_chosen:ExplicitIs0, data=d_pilotB) # to plot: coeff on prop_focal_chosen
kable(tidy(fit))
```

We find that prop_focal_chosen predicts opp_cost_eval_post for the explicit condition but not for the implicit condition which is the opposite of what the researchers in the paper got.

The researchers decided to plot the above relationship opp_cost_eval_post VS prop_focal_chosen so we do the same: 

For the Explicit condition:

```{r, echo=FALSE}
ggplot(data = d_explicit, aes(x=prop_focal_chosen, y=opp_cost_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Opportunity Cost") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

Similar to the authors, we also get a downward slope for the explicit condition for the graph of Eval of Opportunity Cost vs Prop of Focal Options Chosen. 

For the Implicit condition:

```{r, echo=FALSE}
ggplot(data = d_implicit, aes(x=prop_focal_chosen, y=opp_cost_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Opportunity Cost") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

Like the authors, we get an upward slope for the plot of Eval of Opportunity Cost vs. Proportion of Focal Options Chosen. No confidence interval appears however.

We get similar slopes as the authors but don't always agree on the slope values or which variables are significant predictors of our target variables.  

The authors caught one important bug in our pilot B implementation when we showed it to them for feedback after the pilot was run. They said the choice-intervention questions should have been on separate screens. This may have indeed caused our replication failure for pilot B because our final replication was a success. 
