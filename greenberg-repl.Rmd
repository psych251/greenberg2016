---
title: "Greenberg2016 Replication"
output:
  html_document: default
  html_notebook: default
---

### Replication of Study "Opportunity Cost Neglect Attenuates the Effect of Choices on Preferences" by Adam Eric Greenberg & Stephen A. Spiller (200x, Psychological Science)

Gobi Dasu
gdasu@stanford.edu

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

```{r, include=FALSE}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CARPSreports) # custom report functions
library(stringr)
```

#Introduction

This paper looked at how making opportunity cost explicit at the time of choice affects evaluations of focal options ("available" options) and the opportunity cost itself. The study entailed surveying 101 Mechanical Turk workers about how much (7 point Likert) they like each of 11 offers, one of which was the "opportunity cost", both before (pre-choice) and after (post-choice) a "choice" intervention. The "choice" intervention involved asking participants in an "implicit" group whether-or-not they would choose to accept a particular offer, while asking participants in an "explicit" group whether they would choose to accept a particular offer or decline the offer and accept the "opportunity cost" offer instead (i.e. in our case a $100 VISA card). 

For the evaluation, the researchers looked at how "spread" of preferences across choices is affected by making opportunity cost explicit. The researchers calculated the spread of pre-choice and post-choice responses by taking the mean of the focal option evaluations minus the base option (opportunity cost) evaluation. Spread could be negative or positive, and just represented how far away mean focal option evaluation was from the evaluation of opportunity cost. Based on this definition, one may expect that a participant in the explicit condition who selects many focal options explicitly over the $100 VISA card, would in a post-choice survey evaluate their chosen focal options as even further away from the base option than in their pre-choice survey. To test this, the researchers hence controlled for pre-choice spread and used proportion of focal options chosen and condition as well as their interaction to predict post-choice spread. The interaction of condition and proportion of focal options chosen indeed was a significant predictor of post-choice spread (p=0.023). Moreover, they found that when opportunity costs were explicit, postchoice spread increased with the proportion of focal options chosen (p=0.006), but no significant relationship when the condition was implicit. 

We found ... 

#Methods

Here is a link to the current working experiment: https://stanforduniversity.qualtrics.com/jfe/form/SV_4OWrURMHT8ffmHr

###Power Analysis
Based on the first 4 links from https://www.google.com/search?q=effect+size+linear+regression&oq=effect+size+linear+regression&aqs=chrome.0.69i59l2j0l4.4582j0j4&sourceid=chrome&ie=UTF-8 as well as  https://www.statmethods.net/stats/power.html, R^2 (population squared multiple correlation) is the effect size of a multiple linear regression. After contacting the authors, we were told that observed R^2 for the full model was 0.5621. Adjusted R^2 for the full model was 0.5439. Full model test was F(4, 96) = 30.81, p < .001. Given the sample size of 101, the 3 predictors (1 of which was a tested predictor -- choice), and the pre-specified significance level of alpha err prob = 0.05, we found that the post-hoc power of the experiment was 0.99.

For the model to detect the effect size of 0.5621, with an alpha err prob = 0.05, we found 10 samples were required to achieve 80% power, 12 samples for 90% power, and 13 samples for 95% power. We used G*Power. 

###Planned Sample
Because of the sufficiently large effect size, we planned to use fewer samples than the experimenters did for sake of cost -- 35 participants. We opted for this very high fidelity replication because the task was not very long (only 5 minutes based on our pilot). Like the original experimenters, we sampled US-only mechanical turk workers (80%+ approval rating) and applied no other preselection rules. We thew out samples that failed the attention check and ensured that the difference in comprehension check performance between the two groups was not significant.

###Materials & Procedure
> We assessed prechoice evaluations by asking participants to imagine that they were eligible to receive different independent offers as part of a marketing promotion (e.g., “2 nights in a luxury hotel”). Participants used a 7-point scale (1 = not at all, 7 = very much) to rate how much they would like each of 11 such offers. One of these (a $100 Visa gift card) would be the option that represented the opportunity cost at the time of choice and was either the first offer or the last offer to be evaluated; order was counterbalanced across participants.
After completing the prechoice evaluations, participants were asked to imagine that they had accumulated 50,000 airline miles and that their miles would expire soon. If they did not spend their miles before the miles expired, the airline would send them a $100 Visa gift card. Participants decided whether to spend their airline miles on each of 10 focal offers (e.g., “Spend my miles on 2 nights in a luxury hotel”).2 Each option was presented as an independent decision, and participants were instructed to treat each decision as though it were their only opportunity to spend their miles before they expired. As a result, the 10 focal offers competed not with one another but only with the opportunity cost (i.e., the $100 Visa gift card).
Participants assigned to the implicit condition made choices between accepting the focal options (e.g., “Spend my miles on 2 nights in a luxury hotel”) or rejecting the focal options (e.g., “Do not spend my miles on 2 nights in a luxury hotel”); this frame made opportunity costs implicit. Participants assigned to the explicit condition made choices between accepting the focal options or rejecting the focal options in favor of the explicit opportunity cost instead (e.g., “Do not spend my miles on 2 nights in a luxury hotel and take the $100 Visa gift card instead”; Frederick et al., 2009). The two frames are formally equivalent.
Postchoice evaluations were assessed by asking par- ticipants again to rate the desirability of each of the 11 prechoice offers, using a method identical to that used in the prechoice evaluations. The order in which the oppor- tunity cost was presented was counterbalanced. At the top of the survey screen, participants saw the full list of 10 choices they had made (e.g., “Given the choice of: [focal option], or [nonfocal option], you chose: [chosen option]”). The wording was consistent with the manipu- lation: The description of the nonfocal option included the phrase “and take the $100 Visa gift card instead” only for participants in the explicit condition.
Finally, participants reported what they thought would happen to their miles if they decided not to spend them, completed an attention check, and reported their sex and age."

Our Materials and Procedure were identical to the authors', quoted above.

###Analysis Plan
Firstly, data was anonymized and those who did not pass the attention check were removed. We ensured that the proportion of participants passing the comprehension check did not vary by condition [pending], as did the authors. 

The researchers calculated the spread of pre-choice and post-choice responses by taking the mean of the focal option evaluations minus the base option (opportunity cost) evaluation. They regressed post-choice spread on condition (implicit or explicit) and proportion of focal offers chosen as well as the interaction of the two, controlling for pre-choice spread. They also performed "simple effects" 1-variable linear regressions of (a) post choice spread [Y] VS proportion of focal options chosen [X], (b) evaluation of focal options [Y] VS proportion of focal options chosen [X], and (c) evaluation of opportunity costs [Y] VS proportion of focal options chosen [X]. In (a), prechoice spread was a covariate, but in (b) & (c), only the relevant prechoice evaluation was used as a covariate -- i.e. mean prechoice evaluation of focal options was a covariate for (b), and prechoice evaluation of the opportunity cost was a covariate for (c). 

There were no differences between my analysis methods from the original study.

###(Post Data Collection) Methods Addendum
... will complete after the final mturk study

###Actual Sample
The sample size, demographics, data exclusions followed the rules spelled out in analysis plan.
... will complete rest after the final mturk study

###Differences from pre-data collection methods plan
None

#Results

###Data preparation
Data preparation following the analysis plan.

```{r, include=TRUE}

# note qualtrics download wasn't ideal so had to do Find&Delete of ": Very Much" / ": Not at all" string annotations that were cluttering numerical data
qualtrics_data <- read_csv("qualtrics_pilotAB.csv") %>% 
  rename(consent=Q1) %>%
  rename(prechoice_100VISA_1=Q4) %>% # prechoice
  rename(prechoice_focal_1st=Q7) %>%
  rename(prechoice_focal_last=Q16) %>%
  rename(prechoice_100VISA_2=Q69) %>% 
  rename(choice_explicit_1st=Q18) %>% # choice explicit
  rename(choice_explicit_last=Q27) %>% 
  rename(postchoice_explicit_100VISA_1=Q41) %>%
  rename(postchoice_explicit_focal_1st=Q42) %>%
  rename(postchoice_explicit_focal_last=Q51) %>%
  rename(postchoice_explicit_100VISA_2=Q70) %>%
  rename(choice_implicit_1st=Q30) %>% # choice implicit
  rename(choice_implicit_last=Q39) %>% 
  rename(postchoice_implicit_100VISA_1=Q53) %>%
  rename(postchoice_implicit_focal_1st=Q54) %>%
  rename(postchoice_implict_focal_last=Q63) %>%
  rename(postchoice_implicit_100VISA_2=Q71) %>%
  rename(comprehension_check=Q65) %>%
  rename(attention_check=Q66) %>%
  rename(sex=Q67) %>%
  rename(age=Q68)

# remove 1st 2 rows
qualtrics_data <- qualtrics_data[-c(1,2),]

# calculate prechoice spread, postchoice spread, proportion of focal options chosen, condition
qualtrics_data <- qualtrics_data %>% 
  mutate(condition = ifelse(is.na(qualtrics_data$choice_explicit_1st), -1, 1)) %>%
  mutate(allchoices = do.call(paste0, qualtrics_data[c(31:40, 53:62)])) 
qualtrics_data$prop_focal_opts_chosen <- (10 - str_count(qualtrics_data$allchoices, "Not spend"))/10

qualtrics_data[, c(63:74)] <- sapply(qualtrics_data[, c(63:74)], as.numeric)
qualtrics_data[, c(41:52)] <- sapply(qualtrics_data[, c(41:52)], as.numeric)
qualtrics_data$postchoice_spread <- ifelse(
  qualtrics_data$condition==-1, 
  rowMeans(subset(qualtrics_data, select = c(64:73))) - 
    ifelse(
      is.na(qualtrics_data$postchoice_implicit_100VISA_1),
      qualtrics_data$postchoice_implicit_100VISA_2,
      qualtrics_data$postchoice_implicit_100VISA_1
    ),
  rowMeans(subset(qualtrics_data, select = c(42:51))) - 
    ifelse(
      is.na(qualtrics_data$postchoice_explicit_100VISA_1),
      qualtrics_data$postchoice_explicit_100VISA_2,
      qualtrics_data$postchoice_explicit_100VISA_1
    )
)
qualtrics_data$focal_opts_eval_post = ifelse(
  qualtrics_data$condition==-1, 
  rowMeans(subset(qualtrics_data, select = c(64:73))),
  rowMeans(subset(qualtrics_data, select = c(42:51)))
)
qualtrics_data$opp_cost_eval_post = ifelse(
  qualtrics_data$condition==-1, 
  ifelse(
    is.na(qualtrics_data$postchoice_implicit_100VISA_1),
    qualtrics_data$postchoice_implicit_100VISA_2,
    qualtrics_data$postchoice_implicit_100VISA_1
  ),
  ifelse(
    is.na(qualtrics_data$postchoice_explicit_100VISA_1),
    qualtrics_data$postchoice_explicit_100VISA_2,
    qualtrics_data$postchoice_explicit_100VISA_1
  )
)

qualtrics_data[, c(19:30)] <- sapply(qualtrics_data[, c(19:30)], as.numeric)
qualtrics_data$prechoice_spread <- rowMeans(subset(qualtrics_data, select = c(20:29))) - 
  ifelse(
    is.na(qualtrics_data$prechoice_100VISA_1),  
    qualtrics_data$prechoice_100VISA_2,
    qualtrics_data$prechoice_100VISA_1
  )
qualtrics_data$focal_opts_eval_pre <- rowMeans(subset(qualtrics_data, select = c(20:29)))
qualtrics_data$opp_cost_eval_pre <- ifelse(
  is.na(qualtrics_data$prechoice_100VISA_1),  
  qualtrics_data$prechoice_100VISA_2,
  qualtrics_data$prechoice_100VISA_1
)

d_pilotA <- qualtrics_data[c(7:9,15:47), ] 
d_pilotB <- qualtrics_data[c(72:90), ]

# make sure attention check passed
d_pilotA <- d_pilotA[d_pilotA$attention_check=='Somewhat disagree',]
d_pilotB <- d_pilotB[d_pilotB$attention_check=='Somewhat disagree',] %>% mutate(ImplicitIs0=(condition==1)) %>% mutate(ExplicitIs0=(condition==-1))

```

###Confirmatory analysis
The analyses as specified in the analysis plan

#### PILOT A
```{r}
fit <- lm(postchoice_spread ~ prechoice_spread + condition*prop_focal_opts_chosen, data=d_pilotA)
summary(fit)
```
Now this is very interesting. Like the authors, here I see that PRECHOICE_SPREAD has a p < .001.
Like the authors, I see that PROP_FOCAL_OPT_CHOSEN is significant (their_p = .021, our_p=.044).
What is strange is that our CONDITION and PROP_FOCAL_OPT_CHOSEN:CONDITION p values are exactly 10 times larger than the authors' p=.018 and p=.023. This is quite perplexing. I went through and could not catch any error in my implementation -- I have implemented this pipeline in both excel and R independently, for instance to ensure these analyses are valid.

#### PILOT B
```{r}
fit <- lm(postchoice_spread ~ prechoice_spread + condition*prop_focal_opts_chosen, data=d_pilotB)
summary(fit)
```

Again, like in Pilot A, we see that PRECHOICE_SPREAD and PROP_FOCAL_OPT_CHOSEN are significant predictors of POSTCHOICE_SPREAD, but CONDITION and the interaction of CONDITION and PROP_FOCAL_OPT_CHOSEN are not significant predictors, though the paper claimed they should be. 

I also implemented, for pilot B data, the remaining auxiliary analysis of the paper, such as simple effects models and plots of simple effects models:

```{r}
d_explicit <- d_pilotB[d_pilotB$condition==1,]
d_implicit <- d_pilotB[d_pilotB$condition==-1,]
#fit <- lm(postchoice_spread ~ condition/prop_focal_opts_chosen + prechoice_spread, data=d_pilotB)
#summary(fit)
#fit <- lm(postchoice_spread ~ condition/prop_focal_opts_chosen + prechoice_spread, data=d_implicit)
#summary(fit)

fit <- lm(postchoice_spread ~ prechoice_spread + prop_focal_opts_chosen + ImplicitIs0 + prop_focal_opts_chosen:ImplicitIs0, data=d_pilotB) # to plot: coeff on prop_focal_opts_chosen
summary(fit)
fit <- lm(postchoice_spread ~ prechoice_spread + prop_focal_opts_chosen + ExplicitIs0 + prop_focal_opts_chosen:ExplicitIs0, data=d_pilotB) # to plot: coeff on prop_focal_opts_chosen
summary(fit)
```
We get that prop_focal_opts_chosen predicts postchoice_spread in the implicit and explicit conditions, which does not match the paper in which prop_focal_opts_chosen predicts postchoice_spread only in the explicit condition. However, we see that the prediction is much stronger (2 asterisks) in the explicit condition as opposed to (1 asterisk) in the implicit condition.

```{r}
fit <- lm(focal_opts_eval_post ~ focal_opts_eval_pre + prop_focal_opts_chosen + ImplicitIs0 + prop_focal_opts_chosen:ImplicitIs0, data=d_pilotB) # to plot: coeff on prop_focal_opts_chosen
summary(fit)
fit <- lm(focal_opts_eval_post ~ focal_opts_eval_pre + prop_focal_opts_chosen + ExplicitIs0 + prop_focal_opts_chosen:ExplicitIs0, data=d_pilotB) # to plot: coeff on prop_focal_opts_chosen
summary(fit)
```

The researchers decided to plot the above relationship focal_opts_eval_post VS prop_focal_opts_chosen so we do the same: 

For the Explicit condition: 

```{r}
ggplot(data = d_explicit, aes(x=prop_focal_opts_chosen, y=focal_opts_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Focal Options") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

This plot actually looks quite similar to the results that the authors got for explicit condition. The shaded area is the 95% confidence interval, same as what the authors graphed. 

And for the implicit condition:

```{r}
ggplot(data = d_implicit, aes(x=prop_focal_opts_chosen, y=focal_opts_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Focal Options") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

This plot similar to the results that the authors got for implicit condition, but it seems like we do not have enough data to cover the full graph. The shaded area is the 95% confidence interval, same as what the authors graphed. 

```{r}
fit <- lm(opp_cost_eval_post ~ opp_cost_eval_pre + prop_focal_opts_chosen + ImplicitIs0 + prop_focal_opts_chosen:ImplicitIs0, data=d_pilotB) # to plot: coeff on prop_focal_opts_chosen
summary(fit)
fit <- lm(opp_cost_eval_post ~ opp_cost_eval_pre + prop_focal_opts_chosen + ExplicitIs0 + prop_focal_opts_chosen:ExplicitIs0, data=d_pilotB) # to plot: coeff on prop_focal_opts_chosen
summary(fit)
```

We find that prop_focal_opts_chosen predicts opp_cost_eval_post for the explicit condition but not for the implicit condition which is the opposite of what the researchers in the paper got.

The researchers decided to plot the above relationship opp_cost_eval_post VS prop_focal_opts_chosen so we do the same: 

For the Explicit condition:

```{r}
ggplot(data = d_explicit, aes(x=prop_focal_opts_chosen, y=opp_cost_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Opportunity Cost") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

Similar to the authors, we also get a downward slope for the explicit condition for the graph of Eval of Opportunity Cost vs Prop of Focal Options Chosen. 

For the Implicit condition:

```{r}
ggplot(data = d_implicit, aes(x=prop_focal_opts_chosen, y=opp_cost_eval_post)) + geom_point() + xlim(c(0,1.0)) + ylim(c(3,7)) + ylab("Evaluation of Opportunity Cost") + xlab("Proportion of Focal Options Chosen") + geom_smooth(method="lm") 
```

Like the authors, we get an upward slope for the plot of Eval of Opportunity Cost vs. Proportion of Focal Options Chosen. No confidence interval appears however.

It is quite interesting how we get similar plots to the authors but don't always agree on which variables are significant predictors of our target variables.  

... sections below will be completed after final mturk study ...

###Exploratory analyses
... may add after final mturk study

#Discussion

#Summary of Replication Attempt
Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

So far from Pilot A, the result is partially replicated.
So far from Pilot B, the replication is mostly a failure with some findings partially replicated.

#Commentary
Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.

